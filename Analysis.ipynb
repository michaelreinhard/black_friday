{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from collections import OrderedDict\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a link to the data: [Black Friday](https://www.kaggle.com/mehdidag/black-friday). Here is the original posting of the data set from [Analytics Vidhya](https://datahack.analyticsvidhya.com/contest/black-friday/#problem_statement). The data is from Indian department stores and is posted on the website for use in a contest. The contest is to predict purchases and that will be one of the business questions I analyze in this notebook. \n",
    "\n",
    "[Here is a Medium Blogpost](https://medium.com/@michaelrobertreinhard/men-are-doing-the-shopping-depending-on-the-product-63393536595c) based on this analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "zip_ref = zipfile.ZipFile(\"BlackFriday.csv.zip\", 'r')\n",
    "zip_ref.extractall()\n",
    "zip_ref.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('BlackFriday.csv', sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Wrangling\n",
    "As this data was taken from a contest it is, in the main, ready for analysis, but there are a few things that need to be done to it. The missing values have to be dealt with and some variables need their type adjusted.\n",
    "### Missing Values\n",
    "First I look for missing values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The missing values appear to be in the Product Categories 2 and 3. My guess is that these are the subcategories of product 1 so when they are missing I will just the product category 1.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#substitue Product Category 1 for missing data in 2 and 3\n",
    "df['Product_Category_2'] = df['Product_Category_2'].fillna(\n",
    "    df['Product_Category_1']) \n",
    "df['Product_Category_3'] = df['Product_Category_3'].fillna(\n",
    "    df['Product_Category_1']) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mis-Typed data\n",
    "There are some data that have the wrong type. For instance age is presently coded as a string variable becuase of the categories, like '0-17', '55+', it was coded into. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Age'] = df['Age'].replace(\n",
    "    {'0-17':int(15), \n",
    "     '55+':int(65),\n",
    "     '18-25':int(22),\n",
    "     '26-35':int(31),\n",
    "     '36-45':int(41),\n",
    "     '46-50':int(48),\n",
    "     '51-55':int(53)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same is true for the number of years spent in current city, though this variable only codes up to a maximum of 4 years. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Stay_In_Current_City_Years'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#turn into an integer \n",
    "df['Stay_In_Current_City_Years'] = df['Stay_In_Current_City_Years'].replace(\n",
    "    {'0':int(0), \n",
    "     '1':int(1), \n",
    "     '2':int(2), \n",
    "     '3':int(3), \n",
    "     '4+':int(4)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answering Business Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Are there differences between groups?\n",
    "This is marketing data and the main use it can be put to is singling out groups for special marketing efforts. Therefore, the main group of hypotheses to test are whether there are differences in purchases among various groups. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('Occupation')['Purchase'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That looks like some very small differences, too small to be statistically significant. But to be sure lets apply a t test to these differences. I first obtain the standart error for the group difference and then test to flag any group differences that are more than two standard deviations from the mean for the entire data set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = df['Purchase'].mean()\n",
    "standard_deviation = df['Purchase'].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sig_occ_groups = []\n",
    "for occ_group in df.groupby('Occupation')['Purchase'].mean():\n",
    "#     print(abs(mean - 2*standard_deviation) < abs(mean - occ_group))\n",
    "    if abs(mean - 2*standard_deviation) < abs(mean - occ_group):\n",
    "        sig_occ_groups.append(occ_group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sig_occ_groups)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the first business hypothesis, that purchases differ significantly by occupation, is shown to be false.  \n",
    "\n",
    "But this exercise points to a procedure that could be usefully automated. We seek to find any groups that differ in a statistically significant way from the mean of the data set in purchases. Why not make the above procedure a functtion that could be applied to all of the groups in the data set? That is what I am going to do. The function will take a category of consumers, group the data set by them, and then test to see if the groups differ from the data set mean in a statistically significant way.  \n",
    "\n",
    "I will assume the data set mean and standard deviations to be globally available to the data set in this analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_diff(df, group, mean, std_dev):\n",
    "    sig_groups = []\n",
    "    for index, group_mean in enumerate(df.groupby(group)['Purchase'].mean()):\n",
    "        if abs(mean - 2*std_dev) < abs(mean - group_mean):\n",
    "            sig_groups.append(index)\n",
    "    return sig_groups\n",
    "        \n",
    "occupation_groups = group_diff(df=df, group='Gender', \n",
    "                               mean=df['Purchase'].mean(), \n",
    "                               std_dev=df['Purchase'].std())\n",
    "print(occupation_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_groups = group_diff(df=df, group='Gender', \n",
    "                           mean=df['Purchase'].mean(), \n",
    "                           std_dev=df['Purchase'].std())\n",
    "print(gender_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_groups = group_diff(df=df, group='City_Category', \n",
    "                         mean=df['Purchase'].mean(), \n",
    "                         std_dev=df['Purchase'].std())\n",
    "print(city_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "marital_groups = group_diff(df=df, group='Marital_Status', \n",
    "                         mean=df['Purchase'].mean(), \n",
    "                         std_dev=df['Purchase'].std())\n",
    "print(marital_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can give groupby lists allowing up to test subgroups\n",
    "gender_maritalStatus_groups = group_diff(df=df, group=['Gender', 'Marital_Status'], \n",
    "                                         mean=df['Purchase'].mean(), \n",
    "                                         std_dev=df['Purchase'].std())\n",
    "\n",
    "print(gender_maritalStatus_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Age_groups = group_diff(df=df, group='Age', \n",
    "                        mean=df['Purchase'].mean(), \n",
    "                        std_dev=df['Purchase'].std())\n",
    "print(Age_groups)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So this gives a fuller answer to our first business question, are there any significant differences between groups in their purchases? The answer is no, at least for this data set. In all the demographic groups we have the differences in mean purchases by group are not statistically significant. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Product Categories\n",
    "But if there are no statistically significant differences between groups in the average amount of purchases they make, does that hold true for every product category? Here I will only ask the question for one demographic distinction: gender."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "male_means = df[df['Gender']=='M'].groupby(['Product_Category_1', \n",
    "                                            'Gender'])['Purchase'].count()\\\n",
    "                                            /len(df[df['Gender']=='M'])\n",
    "    \n",
    "female_means = df[df['Gender']=='F'].groupby(['Product_Category_1', \n",
    "                                              'Gender'])['Purchase'].count()\\\n",
    "                                              /len(df[df['Gender']=='F'])\n",
    "\n",
    "male_stderr = df[df['Gender']=='M'].groupby(['Product_Category_1', \n",
    "                                             'Gender'])['Purchase'].std()\\\n",
    "                                             /len(df[df['Gender']=='M'])\n",
    "    \n",
    "female_stderr = df[df['Gender']=='F'].groupby(['Product_Category_1', \n",
    "                                               'Gender'])['Purchase'].std()\\\n",
    "                                               /len(df[df['Gender']=='F'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.pyplot import figure\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "plt.clf()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ind = df['Product_Category_1'].unique()\n",
    "ind = np.array(sorted(ind))\n",
    "\n",
    "fig.set_size_inches(14, 8)\n",
    "\n",
    "width = 0.40  \n",
    "\n",
    "p1 = plt.bar(ind, male_means, width, yerr=male_stderr, label='Male')\n",
    "p2 = plt.bar(ind + width, female_means, width, yerr=female_stderr, label='Female')\n",
    "\n",
    "ax.set_xlabel('Product Catogories')\n",
    "ax.set_ylabel('Average Number of Purchases by Gender')\n",
    "plt.title('Purchases by Product Category and Gender')\n",
    "ax.set_xticks(ind + width/2)\n",
    "ax.set_xticklabels(('1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18'))\n",
    "\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can see the differences between the purchases of men and women by product category. We can see by the overlap of the error bars that the differences are not statistically significant in all but three categories, the three with the greatest differences in the means for men and women. Lets adjust our chart to emphasize those three categories of product purchases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff = male_means.values - female_means.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_dict = {}\n",
    "for i,j in zip([x for x in range(1,19)],diff):\n",
    "    orig_dict[i] = j\n",
    "print(orig_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ord_dict = OrderedDict()\n",
    "for i,j in zip(diff, [x for x in range(1,19)]):\n",
    "    ord_dict[i] = j\n",
    "print(ord_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I have the values in an ordered dict with their indexes, the category numbers, as thier values. Next, I want to create a new dictionary with those same values as keys, but this time with thier absolute values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abs_val_dict = dict()\n",
    "for key, value in ord_dict.items():\n",
    "    abs_val_dict[abs(key)] = value\n",
    "print(abs_val_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I use the absolute values to sort the dictionary values by the absolute value of their keys and I will save this to another data structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_list = []\n",
    "for i in sorted(abs_val_dict.keys(),reverse=True):\n",
    "    category_list.append(abs_val_dict[i])\n",
    "print(category_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then I use this list of category names to get the original values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = []\n",
    "for i in category_list:\n",
    "    values.append(orig_dict[i])\n",
    "\n",
    "values_arr = np.array(values)\n",
    "category_arr = np.array(category_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_arr_str = [str(i) for i in category_arr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "plt.cla()\n",
    "\n",
    "fig = plt.gcf()\n",
    "\n",
    "fig.set_size_inches(14, 8)\n",
    "\n",
    "width = 0.70\n",
    "\n",
    "plt.barh(category_arr_str[::-1], values_arr[::-1], width)\n",
    "plt.xlabel('Difference in Proportion')\n",
    "plt.xticks=category_arr_str\n",
    "\n",
    "\n",
    "plt.ylabel('Product Category')\n",
    "\n",
    "\n",
    "plt.annotate(xy = [-0.04, 2], s='Women', fontsize=30)\n",
    "plt.annotate(xy = [0.04, 2], s='Men', fontsize=30)\n",
    "plt.title('Biggest Differences in Male and Female Spending by Category');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So from this graph it is easy to see that product category 1 is favored by men by a nearly 10% margin, while categories 8 and 5 are favored by women by around 5%. This is something to be taken into account in business decisions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prep for machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['Purchase']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop unused variables\n",
    "Now that we have put purchase in the 'y' or target data we have to get rid of it. I experimented with leaving out other variables and so wrote a function that defaults to removing 'purchase', 'User_ID', and 'Product_ID'. I left these out because they are not really general enough to provide an real insights to policy. Predicting how much a person spend from the product they bought is largely an exercise in predicting the price of the product. The same problem presents itself with the User_ID. We can predict the individual's behavior but can't generalise from that without knowing some characteristics that person shares with other. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make a function to drop unused categories\n",
    "def drop_col(df, use_product_factor=False, category=False):\n",
    "    '''drop categories that are unused, making User_ID \n",
    "       and Product_ID optional and making Product_Category_ID optional'''\n",
    "    \n",
    "    if use_product_factor:   \n",
    "        my_list = ['Purchase', \n",
    "                   'User_ID', \n",
    "                   'Product_ID']\n",
    "    else:\n",
    "        my_list = ['Purchase']\n",
    "    \n",
    "    if category:\n",
    "        my_list.extend(['Product_Category_1', \n",
    "                        'Product_Category_2', \n",
    "                        'Product_Category_3'])\n",
    "    \n",
    "    for col in my_list:\n",
    "        try:\n",
    "            df.drop(col, inplace=True, axis=1)\n",
    "        except:\n",
    "            'column has already been deleted'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_col(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare list of qualitative variables\n",
    "The next thing we have to do is get the qualitative variables into a data frame that we can work with. Here I have created a function that defaults to four qualitative variables and includes the option to add product ID and the Product Categories. I have found that the default is the most informative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_qual_var(df, Product_ID=False, Product_Category=True):\n",
    "    '''creates list of qualitative variables. Includes product category and\n",
    "    excludes Product_ID by default'''\n",
    "    \n",
    "    my_list = ['Occupation', 'Marital_Status', 'Gender', 'City_Category']\n",
    "    \n",
    "    if Product_ID==True:\n",
    "        my_list.append('Product_ID')\n",
    "    if Product_Category==True:\n",
    "        my_list.extend(['Product_Category_1', 'Product_Category_2', 'Product_Category_3'])\n",
    "    df_qual = df.loc[:, my_list]\n",
    "    \n",
    "    return df_qual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_qual = select_qual_var(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get dummies\n",
    "Now I have to get the dummy variables from the qualitative variables and drop the original qualitative variables. I do this in a for loop. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make dummy variables of the qualitative variables\n",
    "#and drop the original variable\n",
    "for var in df_qual.columns:\n",
    "    df_qual = pd.concat(\n",
    "                [df_qual.drop(var, axis=1), \n",
    "                 pd.get_dummies(df_qual[var], \n",
    "                   drop_first=True, \n",
    "                   prefix=var, \n",
    "                   prefix_sep='_')], \n",
    "                 axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_qual.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get quantitative variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_quant = df.select_dtypes(['float', 'int'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_quant.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Join the two data frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_quant.join(df_qual)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train test split\n",
    "I create a function to do this because I wanted to use a sample of the data frame for experimentation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_test_split(df, y, sample=False):\n",
    "    '''performs train_test_split with option to \n",
    "    create a smaller sample data set of 5000.'''\n",
    "    \n",
    "    if sample:\n",
    "        df = df.sample(n=5000, random_state=42)\n",
    "        y = y.sample(n=5000, random_state=42)\n",
    "    \n",
    "    X = df\n",
    "    y = y\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.3, random_state=42)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = get_train_test_split(df, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale the variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "sc = StandardScaler()  \n",
    "X_train = sc.fit_transform(X_train)  \n",
    "X_test = sc.transform(X_test)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Model\n",
    "I finally present the final model to answer the question how well can we predict purchases. I have left the trial and error for the notebook \"Black Friday\", included in this repository. There I went through Linear Regression and it regularized variants, lasso and ridge regression, and finally onto Random Forest Regression, the results of which, I present here.  \n",
    "\n",
    "Using grid search, I manipulated the maximum depth, features, and leaf nodes, as well as the minimum samples to form a leaf and the number of estimators. I present the results of the best model here. \n",
    "\n",
    "There was little loss in performance of the model from the training to the test data, with the $R^2$ dropping from 65.20 to 65.07. That means we can explain 65% of the variance in purchases by the model. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params={'max_depth': [21], 'max_features': [69], 'max_leaf_nodes': [695],\n",
    "        'min_samples_leaf': [1], 'n_estimators': [500]}\n",
    "cv = GridSearchCV(estimator=RandomForestRegressor(random_state=42), \n",
    "                  param_grid=params, verbose=1, cv=3, n_jobs=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "\n",
    "results = cv.fit(X_train,y_train)\n",
    "\n",
    "t1 = time.time()\n",
    "\n",
    "total = t1 - t0\n",
    "print(total/60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results.best_estimator_)\n",
    "print(results.best_score_)\n",
    "print(results.best_params_)\n",
    "y_test_pred = cv.predict(X_test)\n",
    "\n",
    "print(r2_score(y_test, y_test_pred))\n",
    "print(np.sqrt(mean_squared_error(y_test, y_test_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
